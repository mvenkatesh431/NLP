{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaJllraN5Efx549kmdmwFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenkatesh431/NLP/blob/master/Tensorflow_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshBLVMobAP0",
        "colab_type": "text"
      },
      "source": [
        "- Lets start with the Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gR9KPwXXfbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d012bb8-70ac-4c5b-aa16-c6d580385f26"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(word_index)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tokenizer': 1, 'tensorflow': 2, 'nlp': 3, 'is': 4, 'really': 5, 'cool': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nKqPSWZfVm",
        "colab_type": "text"
      },
      "source": [
        "- Note : Above tokenizer have small 't' but keras takes care of this. and also removes special characters also.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12zm7VIxbGTZ",
        "colab_type": "text"
      },
      "source": [
        "### More on tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEUjeau6ZYyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "30cd02aa-2f5b-43ac-b04f-65a1a83a88ba"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tokenizer': 1, 'tensorflow': 2, 'is': 3, 'nlp': 4, 'really': 5, 'cool': 6, 'awesome': 7, 'spliting': 8, 'sentences': 9, 'in': 10, 'to': 11, 'words': 12, 'using': 13}\n",
            "[[2, 4, 1], [1, 3, 5, 6], [2, 3, 7], [8, 9, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXAvYBu0b4X2",
        "colab_type": "text"
      },
      "source": [
        "-Now we can see that texts_to_sequences directly converted sentences into the Vectors.\n",
        "- Good thing about tokenizer is, It can fit on one data and still we can perform on other data. \n",
        "- What I mean is we can fit the tokenizer on the training data and then only use the transform on the test data. \n",
        "- We must need to do the fit on the training data and only do the transform on the testing data, because we are building model using the word_indexes of training data and if we use the fit on the test data word_indexes will change and effectiveness of model may decrease.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROBgXpoTcvLT",
        "colab_type": "text"
      },
      "source": [
        "### Using tokenizer on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHg-98BLcrJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d4030504-63cc-45a6-f0c0-291160ff77f6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "test_data = [\n",
        "             'NLP Tokenizer became really powerful',\n",
        "             'Tensorflow tokenizer at work'\n",
        "]\n",
        "\n",
        "# Now i am not going to use fit on the test data, But only using the transform\n",
        "seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"Test Seq: \", seq)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'tokenizer': 1, 'tensorflow': 2, 'is': 3, 'nlp': 4, 'really': 5, 'cool': 6, 'awesome': 7, 'spliting': 8, 'sentences': 9, 'in': 10, 'to': 11, 'words': 12, 'using': 13}\n",
            "[[2, 4, 1], [1, 3, 5, 6], [2, 3, 7], [8, 9, 1]]\n",
            "Test Seq:  [[4, 1, 5], [2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gps5J6Qdpqj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   We can see we only got the word index for the words which are in the training vocabulary \n",
        "*   For example : 'NLP Tokenizer became really powerful' is encoded as [4, 1,5 ] where [4-NLP, 1-tokenizer, 5-really] and remaining words which are not present in the training data is ignored.\n",
        "* So tokenizer will only transform the words which are present in the training data, So it is always good idea to have large corpus of training data so that we can have all words in the vocabulary.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3AtBzcveqOj",
        "colab_type": "text"
      },
      "source": [
        "### Solving Out of vocabulary words problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X4Y7CVjcrNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e759a2c8-ebb5-4dec-93a5-37abca14738d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "# for the Tokenizer constructor we can pass the out of vocabulary token (oov_token), If tokenizer gets any oov word it will replace that word with the specified token\n",
        "tokenizer = Tokenizer(num_words = 10, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "test_data = [\n",
        "             'NLP Tokenizer became really powerful',\n",
        "             'Tensorflow tokenizer at work'\n",
        "]\n",
        "\n",
        "# Now i am not going to use fit on the test data, But only using the transform\n",
        "seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"Test Seq: \", seq)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'tokenizer': 2, 'tensorflow': 3, 'is': 4, 'nlp': 5, 'really': 6, 'cool': 7, 'awesome': 8, 'spliting': 9, 'sentences': 10, 'in': 11, 'to': 12, 'words': 13, 'using': 14}\n",
            "[[3, 5, 2], [2, 4, 6, 7], [3, 4, 8], [9, 1, 1, 1, 1, 1, 2]]\n",
            "Test Seq:  [[5, 2, 1, 6, 1], [3, 2, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAD79i7wfWpW",
        "colab_type": "text"
      },
      "source": [
        "For the Tokenizer constructor we can pass the out of vocabulary token (oov_token), If tokenizer gets any oov word it will replace that word with the specified token\n",
        "\n",
        "Now our test seq have <OOV> also, This is one way fill the missing words.\n",
        "\n",
        "**Note:** Make sure to use Unique word for the oov_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD-TO-sZgF1Q",
        "colab_type": "text"
      },
      "source": [
        "### Text Padding \n",
        "\n",
        "- It is good idea to have the same length for all our sentences so that neural network can perform the calculations efficiently\n",
        "- But our sentences may have different length, So we can use the text padding technique to bring all of them in to the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_raH-CyfwNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "28e29e56-28ea-45bf-d6dc-6dffae20ba01"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "# for the Tokenizer constructor we can pass the out of vocabulary token (oov_token), If tokenizer gets any oov word it will replace that word with the specified token\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "padded_seq = pad_sequences(sequences)\n",
        "print(\"Padded Seq :  \\n\", padded_seq)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'tokenizer': 2, 'tensorflow': 3, 'is': 4, 'nlp': 5, 'really': 6, 'cool': 7, 'awesome': 8, 'spliting': 9, 'sentences': 10, 'in': 11, 'to': 12, 'words': 13, 'using': 14}\n",
            "[[3, 5, 2], [2, 4, 6, 7], [3, 4, 8], [9, 10, 11, 12, 13, 14, 2]]\n",
            "Padded Seq :  \n",
            " [[ 0  0  0  0  3  5  2]\n",
            " [ 0  0  0  2  4  6  7]\n",
            " [ 0  0  0  0  3  4  8]\n",
            " [ 9 10 11 12 13 14  2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0H0gkyrg8fR",
        "colab_type": "text"
      },
      "source": [
        "- Now our sentences are transformed in the matrix, Each row represents to one sentence and have the same length\n",
        "- padded sentence length is equal to the maximum length sentence in the training data i.e 4th sentence in the training data have highest length with 7 words, So all other sentences are padded with zeros to make them 7 words.\n",
        "- So basically pad_sequences takes the highest length and converts all other sentences to the \n",
        "- Also note zeros are added at the begining of the sentences. This is also called pre-padding.\n",
        "\n",
        "We can also pad at the end using the padding = 'post' for the pad_sequences function like below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJAWnH51mbkf",
        "colab_type": "text"
      },
      "source": [
        "#### Post Padding, Maxlen and truncating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAs1kwWocrRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "5f70fbd5-1cca-4996-cb2d-dd959ef6f8b5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "# for the Tokenizer constructor we can pass the out of vocabulary token (oov_token), If tokenizer gets any oov word it will replace that word with the specified token\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "padded_seq = pad_sequences(sequences, padding= 'post', maxlen=6)\n",
        "print(\"Padded Seq :  \\n\", padded_seq)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'tokenizer': 2, 'tensorflow': 3, 'is': 4, 'nlp': 5, 'really': 6, 'cool': 7, 'awesome': 8, 'spliting': 9, 'sentences': 10, 'in': 11, 'to': 12, 'words': 13, 'using': 14}\n",
            "[[3, 5, 2], [2, 4, 6, 7], [3, 4, 8], [9, 10, 11, 12, 13, 14, 2]]\n",
            "Padded Seq :  \n",
            " [[ 3  5  2  0  0  0]\n",
            " [ 2  4  6  7  0  0]\n",
            " [ 3  4  8  0  0  0]\n",
            " [10 11 12 13 14  2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzvFIuUBjqUA",
        "colab_type": "text"
      },
      "source": [
        "- Now we can see sentences are padded at the end.\n",
        "- We can also control the length of the words, Observe the 4th sentence only have 6 words and word at the begining(same like padding) has been dropped.  So we are loosing the data by using the maxlen. So choose the maxlen parameter as per your sentences\n",
        "\n",
        "We can also choose which way we can loose the data in case if we use small maxlen parameter. Too choose use truncating parameter like below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FsDJfakcrQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "ec4e1691-fe5a-46d7-e06d-a874c72e7bee"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "# for the Tokenizer constructor we can pass the out of vocabulary token (oov_token), If tokenizer gets any oov word it will replace that word with the specified token\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "padded_seq = pad_sequences(sequences, padding= 'post', truncating='post', maxlen=6)\n",
        "print(\"Padded Seq :  \\n\", padded_seq)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'tokenizer': 2, 'tensorflow': 3, 'is': 4, 'nlp': 5, 'really': 6, 'cool': 7, 'awesome': 8, 'spliting': 9, 'sentences': 10, 'in': 11, 'to': 12, 'words': 13, 'using': 14}\n",
            "[[3, 5, 2], [2, 4, 6, 7], [3, 4, 8], [9, 10, 11, 12, 13, 14, 2]]\n",
            "Padded Seq :  \n",
            " [[ 3  5  2  0  0  0]\n",
            " [ 2  4  6  7  0  0]\n",
            " [ 3  4  8  0  0  0]\n",
            " [ 9 10 11 12 13 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di-6TQ1Pk9ZW",
        "colab_type": "text"
      },
      "source": [
        "- Now we can see last word in the 4th sentence is dropped. i.e tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXJ1XzGnf99",
        "colab_type": "text"
      },
      "source": [
        "Lets apply padding for the test data as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhrQlqaLcrMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "b573a0c9-0846-4847-8611-f22938d7d377"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "words = [\n",
        "    'Tensorflow NLP Tokenizer', \n",
        "    'tokenizer! is really cool',\n",
        "    'Tensorflow is awesome!',\n",
        "    'spliting sentences in to words using tokenizer'\n",
        "]\n",
        "\n",
        "# for the Tokenizer constructor we can pass the out of vocabulary token (oov_token), If tokenizer gets any oov word it will replace that word with the specified token\n",
        "tokenizer = Tokenizer(num_words = 20, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(words)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# fit_on_sequences takes direct sentences, And it will work with the previous fitted data.\n",
        "sequences = tokenizer.texts_to_sequences(words)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "\n",
        "padded_seq = pad_sequences(sequences, padding= 'post', truncating='post', maxlen=6)\n",
        "print(\"\\nPadded Seq :  \\n\", padded_seq)\n",
        "\n",
        "test_data = [\n",
        "             'NLP Tokenizer became really powerful',\n",
        "             'Tensorflow tokenizer at work'\n",
        "]\n",
        "\n",
        "# Now i am not going to use fit on the test data, But only using the transform\n",
        "seq = tokenizer.texts_to_sequences(test_data)\n",
        "padded_test_seq = pad_sequences(seq, padding= 'post', truncating='post', maxlen=6)\n",
        "print(\"\\nTest Seq: \", seq)\n",
        "print(\"\\nPadded test seq : \\n\", padded_test_seq)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'tokenizer': 2, 'tensorflow': 3, 'is': 4, 'nlp': 5, 'really': 6, 'cool': 7, 'awesome': 8, 'spliting': 9, 'sentences': 10, 'in': 11, 'to': 12, 'words': 13, 'using': 14}\n",
            "[[3, 5, 2], [2, 4, 6, 7], [3, 4, 8], [9, 10, 11, 12, 13, 14, 2]]\n",
            "\n",
            "Padded Seq :  \n",
            " [[ 3  5  2  0  0  0]\n",
            " [ 2  4  6  7  0  0]\n",
            " [ 3  4  8  0  0  0]\n",
            " [ 9 10 11 12 13 14]]\n",
            "\n",
            "Test Seq:  [[5, 2, 1, 6, 1], [3, 2, 1, 1]]\n",
            "\n",
            "Padded test seq : \n",
            " [[5 2 1 6 1 0]\n",
            " [3 2 1 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hpbb9-wcrHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}